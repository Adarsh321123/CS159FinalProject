(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ export CUDA_VISIBLE_DEVICES=2
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_lean_copilot_bot.sh
Script executed from: /home/adarsh/ReProver
[2024-06-02 12:05:55,792] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Cloning https://github.com/lecopivo/SciLean.git
Repo name: lecopivo/SciLean
Deleting existing repository directory: /raid/adarsh/repos/lecopivo/SciLean
Cloning into '/raid/adarsh/repos/lecopivo/SciLean'...
remote: Enumerating objects: 14952, done.
remote: Counting objects: 100% (1010/1010), done.
remote: Compressing objects: 100% (296/296), done.
remote: Total 14952 (delta 824), reused 830 (delta 710), pack-reused 13942
Receiving objects: 100% (14952/14952), 9.48 MiB | 10.98 MiB/s, done.
Resolving deltas: 100% (10922/10922), done.
Cloning https://github.com/leanprover-community/aesop.git
Repo name: leanprover-community/aesop
Deleting existing repository directory: /raid/adarsh/repos/leanprover-community/aesop
Cloning into '/raid/adarsh/repos/leanprover-community/aesop'...
remote: Enumerating objects: 6936, done.
remote: Counting objects: 100% (1276/1276), done.
remote: Compressing objects: 100% (266/266), done.
remote: Total 6936 (delta 1076), reused 1129 (delta 989), pack-reused 5660
Receiving objects: 100% (6936/6936), 3.35 MiB | 2.99 MiB/s, done.
Resolving deltas: 100% (5031/5031), done.
Cloning https://github.com/kmill/lean4-raytracer.git
Repo name: kmill/lean4-raytracer
Deleting existing repository directory: /raid/adarsh/repos/kmill/lean4-raytracer
Cloning into '/raid/adarsh/repos/kmill/lean4-raytracer'...
remote: Enumerating objects: 156, done.
remote: Counting objects: 100% (156/156), done.
remote: Compressing objects: 100% (93/93), done.
remote: Total 156 (delta 75), reused 128 (delta 52), pack-reused 0
Receiving objects: 100% (156/156), 10.06 MiB | 9.54 MiB/s, done.
Resolving deltas: 100% (75/75), done.
Cloning https://github.com/AlexKontorovich/PrimeNumberTheoremAnd.git
Repo name: AlexKontorovich/PrimeNumberTheoremAnd
Deleting existing repository directory: /raid/adarsh/repos/AlexKontorovich/PrimeNumberTheoremAnd
Cloning into '/raid/adarsh/repos/AlexKontorovich/PrimeNumberTheoremAnd'...
remote: Enumerating objects: 5560, done.
remote: Counting objects: 100% (2340/2340), done.
remote: Compressing objects: 100% (591/591), done.
remote: Total 5560 (delta 1830), reused 1915 (delta 1729), pack-reused 3220
Receiving objects: 100% (5560/5560), 1.55 MiB | 4.37 MiB/s, done.
Resolving deltas: 100% (3893/3893), done.
Cloning https://github.com/leanprover-community/lean-perfectoid-spaces.git
Repo name: leanprover-community/lean-perfectoid-spaces
Cloning into '/raid/adarsh/repos/leanprover-community/lean-perfectoid-spaces'...
remote: Enumerating objects: 6779, done.
remote: Counting objects: 100% (317/317), done.
remote: Compressing objects: 100% (107/107), done.
remote: Total 6779 (delta 197), reused 315 (delta 196), pack-reused 6462
Receiving objects: 100% (6779/6779), 12.00 MiB | 7.78 MiB/s, done.
Resolving deltas: 100% (4919/4919), done.
Failed to clone leanprover-community/lean-perfectoid-spaces because of HTTP Error 404: Not Found
Cloning https://github.com/flypitch/flypitch.git
Repo name: flypitch/flypitch
Cloning into '/raid/adarsh/repos/flypitch/flypitch'...
remote: Enumerating objects: 3574, done.
remote: Counting objects: 100% (121/121), done.
remote: Compressing objects: 100% (69/69), done.
remote: Total 3574 (delta 68), reused 84 (delta 52), pack-reused 3453
Receiving objects: 100% (3574/3574), 1.79 MiB | 4.39 MiB/s, done.
Resolving deltas: 100% (2689/2689), done.
Failed to clone flypitch/flypitch because of HTTP Error 404: Not Found
Cloning https://github.com/jesse-michael-han/lean-gptf.git
Repo name: jesse-michael-han/lean-gptf
Cloning into '/raid/adarsh/repos/jesse-michael-han/lean-gptf'...
remote: Enumerating objects: 266, done.
remote: Counting objects: 100% (22/22), done.
remote: Compressing objects: 100% (10/10), done.
remote: Total 266 (delta 14), reused 12 (delta 12), pack-reused 244
Receiving objects: 100% (266/266), 51.42 KiB | 438.00 KiB/s, done.
Resolving deltas: 100% (124/124), done.
Failed to clone jesse-michael-han/lean-gptf because of HTTP Error 404: Not Found
Cloning https://github.com/teorth/pfr.git
Repo name: teorth/pfr
Deleting existing repository directory: /raid/adarsh/repos/teorth/pfr
Cloning into '/raid/adarsh/repos/teorth/pfr'...
remote: Enumerating objects: 8182, done.
remote: Counting objects: 100% (285/285), done.
remote: Compressing objects: 100% (90/90), done.
remote: Total 8182 (delta 213), reused 255 (delta 194), pack-reused 7897
Receiving objects: 100% (8182/8182), 4.47 MiB | 6.85 MiB/s, done.
Resolving deltas: 100% (5910/5910), done.
Cloning https://github.com/TwoFX/sudoku.git
Repo name: TwoFX/sudoku
Cloning into '/raid/adarsh/repos/TwoFX/sudoku'...
remote: Enumerating objects: 1316, done.
remote: Counting objects: 100% (385/385), done.
remote: Compressing objects: 100% (151/151), done.
remote: Total 1316 (delta 254), reused 364 (delta 234), pack-reused 931
Receiving objects: 100% (1316/1316), 1.14 MiB | 3.08 MiB/s, done.
Resolving deltas: 100% (851/851), done.
Failed to clone TwoFX/sudoku because of HTTP Error 404: Not Found
Cloning https://github.com/blanchette/logical_verification_2020.git
Repo name: blanchette/logical_verification_2020
Cloning into '/raid/adarsh/repos/blanchette/logical_verification_2020'...
remote: Enumerating objects: 281, done.
remote: Counting objects: 100% (24/24), done.
remote: Compressing objects: 100% (20/20), done.
remote: Total 281 (delta 10), reused 18 (delta 4), pack-reused 257
Receiving objects: 100% (281/281), 5.57 MiB | 7.82 MiB/s, done.
Resolving deltas: 100% (165/165), done.
Failed to clone blanchette/logical_verification_2020 because of HTTP Error 404: Not Found
Cloning https://github.com/lurk-lab/yatima.git
Repo name: lurk-lab/yatima
Deleting existing repository directory: /raid/adarsh/repos/lurk-lab/yatima
Cloning into '/raid/adarsh/repos/lurk-lab/yatima'...
remote: Enumerating objects: 3500, done.
remote: Counting objects: 100% (273/273), done.
remote: Compressing objects: 100% (162/162), done.
remote: Total 3500 (delta 150), reused 196 (delta 109), pack-reused 3227
Receiving objects: 100% (3500/3500), 1.19 MiB | 4.38 MiB/s, done.
Resolving deltas: 100% (2209/2209), done.
2024-06-02 12:07:05.184 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/lurk-lab/yatima', commit='d9f20f51bca748878b8561661fe8bc19a7dba609') relies on an unsupported Lean version: c21d2f29a21a
d17351ebb7353fce9d7c7f6bdca7
Found 6 repositories
Processing /raid/adarsh/repos/lecopivo/SciLean
From https://github.com/lecopivo/SciLean
 * branch            master     -> FETCH_HEAD
Already on 'master'
Your branch is up to date with 'origin/master'.
From https://github.com/lecopivo/SciLean
 * branch            master     -> FETCH_HEAD
Already up to date.
Switched to a new branch '_LeanCopilotBot'
2024-06-02 12:07:06.018 | INFO     | __main__:retrieve_proof:382 - lean toolchain version: {'content': 'leanprover/lean4:v4.7.0\n'}
2024-06-02 12:07:06.018 | INFO     | __main__:retrieve_proof:384 - lean version v: v4.7.0
2024-06-02 12:07:06.018 | INFO     | __main__:retrieve_proof:385 - is supported: True
2024-06-02 12:07:06.018 | INFO     | __main__:retrieve_proof:391 - lean path /home/adarsh/.elan/toolchains/leanprover--lean4---4.7.0
2024-06-02 12:07:06.018 | INFO     | __main__:retrieve_proof:397 - Switched to Lean toolchain at: /home/adarsh/.elan/toolchains/leanprover--lean4---4.7.0
2024-06-02 12:07:06.051 | INFO     | __main__:retrieve_proof:398 - lean --version: Lean (version 4.7.0, x86_64-unknown-linux-gnu, commit 6fce8f7d5cd1, Release)

2024-06-02 12:07:06.051 | INFO     | __main__:retrieve_proof:399 - repo: LeanGitRepo(url='https://github.com/lecopivo/SciLean', commit='e21a3ecdc0442bf795ea665d3a4bdbc5be22602d')
2024-06-02 12:07:06.051 | INFO     | lean_dojo.data_extraction.trace:trace:116 - Loading the traced repo from /raid/adarsh/.cache/lean_dojo/lecopivo-SciLean-e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/SciLean
2024-06-02 12:07:08,343 INFO worker.py:1740 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8268
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2851/2851 [04:45<00:00,  9.99it/s]
Following Github server redirection from /repos/leanprover/std4 to /repositories/529900532
2024-06-02 12:12:27.316 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/leanprover/lean4-cli', commit='be8fa79a28b8b6897dce0713ef50e89c4a0f6ef5') relies on an unsupported Lean version: ec94173
5c80dc54c53948e30c428905b6600f95a
Following Github server redirection from /repos/mhuisi/lean4-cli to /repositories/341363356
2024-06-02 12:13:01.460 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/mhuisi/lean4-cli', commit='10d88b52fa8d717fa8e29af3abf0c3a2bf175497') relies on an unsupported Lean version: 41697dcf6ca
b7ec82723ba404f2bda7a4526bb2b
2024-06-02 12:13:17.599 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/xubaiw/CMark.lean', commit='0077cbbaa92abf855fc1c0413e158ffd8195ec77') relies on an unsupported Lean version: 8fc1af650a
d6d31cf766d9bc84119149330e7d4e
2024-06-02 12:13:27.119 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/fgdorais/lean4-unicode-basic', commit='280d75fdfe7be8eb337be7f1bf8479b4aac09f71') relies on an unsupported Lean version:
 0d7051497ea09b2b4a4ef608e371b8f317487c3c
2024-06-02 12:13:30.567 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/mhuisi/lean4-cli', commit='39229f3630d734af7d9cfb5937ddc6b41d3aa6aa') relies on an unsupported Lean version: 216d2460e0a
dec8317fdeeb6f2543cb7442564fd
2024-06-02 12:13:35.514 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/hargonix/LeanInk', commit='2447df5cc6e48eb965c3c3fba87e46d353b5e9f1') relies on an unsupported Lean version: f6cd6c06958
7cfe62dd68cb6330f9ad794a56724
2024-06-02 12:14:45.210 | INFO     | __main__:retrieve_proof:411 - MAIN: about to split data
2024-06-02 12:14:52.061 | INFO     | __main__:split_data:132 - 72244 theorems in total
2024-06-02 12:14:52.061 | INFO     | __main__:split_randomly:81 - Splitting the theorems randomly
2024-06-02 12:14:52.090 | INFO     | __main__:split_by_premise:94 - Splitting the theorems by premises
2024-06-02 12:14:57.737 | INFO     | __main__:retrieve_proof:413 - MAIN: done splitting data
2024-06-02 12:14:57.737 | INFO     | __main__:retrieve_proof:414 - MAIN: about to export corpus.jsonl
2024-06-02 12:14:57.738 | INFO     | __main__:export_data:178 - Exporting data to path: /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d
2024-06-02 12:14:57.738 | INFO     | __main__:export_premises:144 - /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl already exists. Using existing file
2024-06-02 12:14:57.738 | INFO     | __main__:retrieve_proof:416 - MAIN: exported corpus.jsonl
2024-06-02 12:15:05.125 | INFO     | __main__:retrieve_proof:433 - 28999
2024-06-02 12:15:05.140 | INFO     | __main__:retrieve_proof:447 - Found 293 sorries!
2024-06-02 12:15:05.140 | INFO     | __main__:retrieve_proof:448 - MAIN: about to search for proofs
2024-06-02 12:15:05.140 | INFO     | prover.proof_search:__init__:429 - Launching 5 workers with 1 GPUs.
2024-06-02 12:15:08,517 INFO worker.py:1740 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8267
2024-06-02 12:15:09.468 | INFO     | prover.proof_search:search_unordered:474 - before theorem search:
(pid=877588) [2024-06-02 12:15:14,495] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
(pid=877585)  [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
(pid=877588)  [WARNING]  async_io: please install the libaio-dev package with apt
(pid=877588)  [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
(pid=877588)  [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
(pid=877585)  [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
(pid=877585)  [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
(GpuProver pid=877585) Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../raid/adarsh/kaiyuy_lean
dojo-lean4-retriever-tacgen-byt5-small/model_lightning.ckpt`
(GpuProver pid=877585) 2024-06-02 12:15:16.160 | INFO     | generator.model:__init__:109 - Retriever checkpoint path: /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt
(GpuProver pid=877585) 2024-06-02 12:15:16.160 | INFO     | generator.model:__init__:124 - Loading the retriever from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt
(GpuProver pid=877585) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when p
ossible. If you want to force a new download, use `force_download=True`.
(GpuProver pid=877585)   warnings.warn(
(GpuProver pid=877585) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
(GpuProver pid=877585) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['encoder.shared.weight', 'encoder.encoder.em
bed_tokens.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.o.w
eight', 'encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.encoder.block.0.layer.0.layer_norm.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.0.layer.1.DenseRel
uDense.wi_1.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.0.layer.1.layer_norm.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.k.wei
ght', 'encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.1.layer.0.layer_norm.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encode
r.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.1.layer.1.layer_norm.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.encoder.b
lock.2.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.2.layer.0.layer_norm.weight', 'encoder.encoder.block.2.layer.1.De
nseReluDense.wi_0.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.2.layer.1.layer_norm.weight', 'encoder.encoder.block.3.layer.0.SelfAtten
tion.q.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.3.layer.0.layer_norm.weight', '
encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.3.layer.1.layer_norm.weight', 'encoder
.encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.encoder.blo
ck.4.layer.0.layer_norm.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.4.la
yer.1.layer_norm.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.5.layer.0.SelfAttenti
on.o.weight', 'encoder.encoder.block.5.layer.0.layer_norm.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wo.we
ight', 'encoder.encoder.block.5.layer.1.layer_norm.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.e
ncoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.6.layer.0.layer_norm.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.bl
ock.6.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.6.layer.1.layer_norm.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.7.layer.0.S
elfAttention.v.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.7.layer.0.layer_norm.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense
.wi_1.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.7.layer.1.layer_norm.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.k.weight',
'encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.8.layer.0.layer_norm.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.enco
der.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.8.layer.1.layer_norm.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.9
.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.9.layer.0.layer_norm.weight', 'encoder.encoder.block.9.layer.1.DenseRel
uDense.wi_0.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.9.layer.1.layer_norm.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.
q.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.10.layer.0.layer_norm.weight', 'e
ncoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.10.layer.1.layer_norm.weight', 'enco
der.encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.enco
der.block.11.layer.0.layer_norm.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.encoder
.block.11.layer.1.layer_norm.weight', 'encoder.encoder.final_layer_norm.weight']
(GpuProver pid=877585) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['shared.weight', 'decoder.embed_tokens.weigh
t', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.r
elative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.bl
ock.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight'
, 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight
', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention
.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.la
yer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.l
ayer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.
layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.bl
ock.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.b
lock.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', '
decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_h
ead.weight', 'pytorch-lightning_version', 'global_step', 'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'enco
der.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.la
yer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.laye
r.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.lay
er.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.
block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'enc
oder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight
', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi
_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttent
ion.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.D
enseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.
0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.blo
ck.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.bl
ock.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'enco
der.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'enc
oder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.we
ight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi
_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAt
tention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.D
enseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10
.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'en
coder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weig
ht', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.1.DenseR
eluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight']
(GpuProver pid=877585) Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at kaiyuy/leandojo-lean4-retriever-byt5-small and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decode
r.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.laye
r_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.lay
er.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block
.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.bloc
k.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'dec
oder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.we
ight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k
.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseRel
uDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer
.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.la
yer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder
.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']
(GpuProver pid=877585) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(GpuProver pid=877585) 2024-06-02 12:15:18.542 | INFO     | generator.model:__init__:137 - RetrievalAugmentedGenerator initialized
(GpuProver pid=877585) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['retriever.encoder.shared.weight', 'retrieve
r.encoder.encoder.embed_tokens.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.v.weight',
'retriever.encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'retriever.encoder.encoder.block.0.layer.0.layer_norm.weight', 'retriever.encoder.enco
der.block.0.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.0.layer.1.layer_norm
.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.blo
ck.1.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.1.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_1.weight',
 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.1.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.2.layer.0
.SelfAttention.k.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.2.layer.0.layer_norm.weight', 'retriever.encoder.
encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.2.layer.1.layer_
norm.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder
.block.3.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.3.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weig
ht', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.3.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.4.lay
er.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.4.layer.0.layer_norm.weight', 'retriever.enco
der.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.4.layer.1.la
yer_norm.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'retriever.encoder.enc
oder.block.5.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.5.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.
weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.5.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.6
.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.6.layer.0.layer_norm.weight', 'retriever.
encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.6.layer.
1.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'retriever.encoder
.encoder.block.7.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.7.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.w
i_1.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.7.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.blo
ck.8.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.8.layer.0.layer_norm.weight', 'retrie
ver.encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.8.la
yer.1.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'retriever.enc
oder.encoder.block.9.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.9.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDen
se.wi_1.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.9.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'retriever.encoder.encode
r.block.10.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.10.layer.0.layer_norm.weight'
, 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encode
r.block.10.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.v.weight'
, 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.11.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.11.
layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.11.layer.1.layer_norm.weight', 'retriever.encoder.encoder.final_layer_norm.weight']
(GpuProver pid=877585) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['pytorch-lightning_version', 'global_step',
'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters']
(GpuProver pid=877585) 2024-06-02 12:15:19.792 | INFO     | prover.proof_search:__init__:370 - Loaded model from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning.ckpt
(GpuProver pid=877585) 2024-06-02 12:15:19.793 | INFO     | prover.proof_search:__init__:371 - Using retriever: PremiseRetriever(
(GpuProver pid=877585)   (encoder): T5EncoderModel(
(GpuProver pid=877585)     (shared): Embedding(384, 1472)
(GpuProver pid=877585)     (encoder): T5Stack(
(GpuProver pid=877585)       (embed_tokens): Embedding(384, 1472)
(GpuProver pid=877585)       (block): ModuleList(
(GpuProver pid=877585)         (0): T5Block(
(GpuProver pid=877585)           (layer): ModuleList(
(GpuProver pid=877585)             (0): T5LayerSelfAttention(
(GpuProver pid=877585)               (SelfAttention): T5Attention(
(GpuProver pid=877585)                 (q): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=877585)                 (k): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=877585)                 (v): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=877585)                 (o): Linear(in_features=384, out_features=1472, bias=False)
(GpuProver pid=877585)                 (relative_attention_bias): Embedding(32, 6)
(GpuProver pid=877585)               )
(GpuProver pid=877585)               (layer_norm): T5LayerNorm()
(GpuProver pid=877585)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)             )
(GpuProver pid=877585)             (1): T5LayerFF(
(GpuProver pid=877585)               (DenseReluDense): T5DenseGatedActDense(
(GpuProver pid=877585)                 (wi_0): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=877585)                 (wi_1): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=877585)                 (wo): Linear(in_features=3584, out_features=1472, bias=False)
(GpuProver pid=877585)                 (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)                 (act): NewGELUActivation()
(GpuProver pid=877585)               )
(GpuProver pid=877585)               (layer_norm): T5LayerNorm()
(GpuProver pid=877585)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)             )
(GpuProver pid=877585)           )
(GpuProver pid=877585)         )
(GpuProver pid=877585)         (1-11): 11 x T5Block(
(GpuProver pid=877585)           (layer): ModuleList(
(GpuProver pid=877585)             (0): T5LayerSelfAttention(
(GpuProver pid=877585)               (SelfAttention): T5Attention(
(GpuProver pid=877585)                 (q): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=877585)                 (k): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=877585)                 (v): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=877585)                 (o): Linear(in_features=384, out_features=1472, bias=False)
(GpuProver pid=877585)               )
(GpuProver pid=877585)               (layer_norm): T5LayerNorm()
(GpuProver pid=877585)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)             )
(GpuProver pid=877585)             (1): T5LayerFF(
(GpuProver pid=877585)               (DenseReluDense): T5DenseGatedActDense(
(GpuProver pid=877585)                 (wi_0): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=877585)                 (wi_1): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=877585)                 (wo): Linear(in_features=3584, out_features=1472, bias=False)
(GpuProver pid=877585)                 (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)                 (act): NewGELUActivation()
(GpuProver pid=877585)               )
(GpuProver pid=877585)               (layer_norm): T5LayerNorm()
(GpuProver pid=877585)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)             )
(GpuProver pid=877585)           )
(GpuProver pid=877585)         )
(GpuProver pid=877585)       )
(GpuProver pid=877585)       (final_layer_norm): T5LayerNorm()
(GpuProver pid=877585)       (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=877585)     )
(GpuProver pid=877585)   )
(GpuProver pid=877585) )
(GpuProver pid=877585) 2024-06-02 12:15:19.794 | INFO     | prover.proof_search:__init__:374 - Loading indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl
(GpuProver pid=877585) 2024-06-02 12:15:19.794 | INFO     | common:__init__:200 - Building the corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl
(GpuProver pid=877585) 2024-06-02 12:15:26.825 | INFO     | prover.proof_search:__init__:377 - Loaded indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl
(GpuProver pid=877585) 2024-06-02 12:15:26.826 | INFO     | retrieval.model:reindex_corpus:173 - Re-indexing the retrieval corpus
  0%|          | 0/3381 [00:00<?, ?it/s]
(GpuProver pid=877589) Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../raid/adarsh/kaiyuy_lean
dojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt` [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/use
r-guides/configure-logging.html#log-deduplication for more options.)
(GpuProver pid=877589) 2024-06-02 12:15:16.586 | INFO     | generator.model:__init__:109 - Retriever checkpoint path: /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt [repeated 4x across cluster]
(GpuProver pid=877589) 2024-06-02 12:15:16.586 | INFO     | generator.model:__init__:124 - Loading the retriever from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt [repeated 4x across cluster]
(GpuProver pid=877589) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when p
ossible. If you want to force a new download, use `force_download=True`. [repeated 4x across cluster]
(GpuProver pid=877589)   warnings.warn( [repeated 4x across cluster]
(GpuProver pid=877589) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. [repeated 9x across cluster]
(GpuProver pid=877589) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['encoder.shared.weight', 'encoder.encoder.em
bed_tokens.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.o.w
eight', 'encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.encoder.block.0.layer.0.layer_norm.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.0.layer.1.DenseRel
uDense.wi_1.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.0.layer.1.layer_norm.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.k.wei
ght', 'encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.1.layer.0.layer_norm.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encode
r.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.1.layer.1.layer_norm.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.encoder.b
lock.2.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.2.layer.0.layer_norm.weight', 'encoder.encoder.block.2.layer.1.De
nseReluDense.wi_0.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.2.layer.1.layer_norm.weight', 'encoder.encoder.block.3.layer.0.SelfAtten
tion.q.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.3.layer.0.layer_norm.weight', '
encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.3.layer.1.layer_norm.weight', 'encoder
.encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.encoder.blo
ck.4.layer.0.layer_norm.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.4.la
yer.1.layer_norm.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.5.layer.0.SelfAttenti
on.o.weight', 'encoder.encoder.block.5.layer.0.layer_norm.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wo.we
ight', 'encoder.encoder.block.5.layer.1.layer_norm.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.e
ncoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.6.layer.0.layer_norm.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.bl
ock.6.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.6.layer.1.layer_norm.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.7.layer.0.S
elfAttention.v.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.7.layer.0.layer_norm.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense
.wi_1.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.7.layer.1.layer_norm.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.k.weight',
'encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.8.layer.0.layer_norm.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.enco
der.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.8.layer.1.layer_norm.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.9
.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.9.layer.0.layer_norm.weight', 'encoder.encoder.block.9.layer.1.DenseRel
uDense.wi_0.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.9.layer.1.layer_norm.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.
q.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.10.layer.0.layer_norm.weight', 'e
ncoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.10.layer.1.layer_norm.weight', 'enco
der.encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.enco
der.block.11.layer.0.layer_norm.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.encoder
.block.11.layer.1.layer_norm.weight', 'encoder.encoder.final_layer_norm.weight'] [repeated 4x across cluster]
(GpuProver pid=877589) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['shared.weight', 'decoder.embed_tokens.weigh
t', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.r
elative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.bl
ock.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight'
, 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight
', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention
.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.la
yer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.l
ayer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.
layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.bl
ock.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.b
lock.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', '
decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_h
ead.weight', 'pytorch-lightning_version', 'global_step', 'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'enco
der.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.la
yer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.laye
r.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.lay
er.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.
block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'enc
oder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight
', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi
_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttent
ion.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.D
enseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.
0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.blo
ck.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.bl
ock.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'enco
der.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'enc
oder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.we
ight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi
_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAt
tention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.D
enseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10
.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'en
coder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weig
ht', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.1.DenseR
eluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight'] [repeated 4x across cluster]
(GpuProver pid=877589) Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at kaiyuy/leandojo-lean4-retriever-byt5-small and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decode
r.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.laye
r_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.lay
er.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block
.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.bloc
k.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'dec
oder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.we
ight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k
.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseRel
uDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer
.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.la
yer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder
.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight'] [repeated 4x across cluster]
(GpuProver pid=877589) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. [repeated 4x across cluster]
(GpuProver pid=877589) 2024-06-02 12:15:19.104 | INFO     | generator.model:__init__:137 - RetrievalAugmentedGenerator initialized [repeated 4x across cluster]
(GpuProver pid=877589) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['retriever.encoder.shared.weight', 'retrieve
r.encoder.encoder.embed_tokens.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.v.weight',
'retriever.encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'retriever.encoder.encoder.block.0.layer.0.layer_norm.weight', 'retriever.encoder.enco
der.block.0.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.0.layer.1.layer_norm
.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.blo
ck.1.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.1.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_1.weight',
 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.1.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.2.layer.0
.SelfAttention.k.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.2.layer.0.layer_norm.weight', 'retriever.encoder.
encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.2.layer.1.layer_
norm.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder
.block.3.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.3.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weig
ht', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.3.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.4.lay
er.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.4.layer.0.layer_norm.weight', 'retriever.enco
der.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.4.layer.1.la
yer_norm.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'retriever.encoder.enc
oder.block.5.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.5.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.
weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.5.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.6
.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.6.layer.0.layer_norm.weight', 'retriever.
encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.6.layer.
1.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'retriever.encoder
.encoder.block.7.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.7.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.w
i_1.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.7.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.blo
ck.8.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.8.layer.0.layer_norm.weight', 'retrie
ver.encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.8.la
yer.1.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'retriever.enc
oder.encoder.block.9.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.9.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDen
se.wi_1.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.9.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'retriever.encoder.encode
r.block.10.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.10.layer.0.layer_norm.weight'
, 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encode
r.block.10.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.v.weight'
, 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.11.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.11.
layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.11.layer.1.layer_norm.weight', 'retriever.encoder.encoder.final_layer_norm.weight'] [repeated 4x across
 cluster]
(GpuProver pid=877589) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['pytorch-lightning_version', 'global_step',
'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters'] [repeated 4x across cluster]
(GpuProver pid=877589) 2024-06-02 12:15:20.345 | INFO     | prover.proof_search:__init__:370 - Loaded model from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning.ckpt [repeated 4x across cluster]
(GpuProver pid=877589) 2024-06-02 12:15:20.346 | INFO     | prover.proof_search:__init__:371 - Using retriever: PremiseRetriever( [repeated 4x across cluster]
(GpuProver pid=877589)     (encoder): T5Stack( [repeated 8x across cluster]
(GpuProver pid=877589)     (shared): Embedding(384, 1472) [repeated 4x across cluster]
(GpuProver pid=877589)       (embed_tokens): Embedding(384, 1472) [repeated 4x across cluster]
(GpuProver pid=877589)       (block): ModuleList( [repeated 4x across cluster]
(GpuProver pid=877589)             (1): T5LayerFF( [repeated 20x across cluster]
(GpuProver pid=877589)           (layer): ModuleList( [repeated 8x across cluster]
(GpuProver pid=877589)               (SelfAttention): T5Attention( [repeated 8x across cluster]
(GpuProver pid=877589)                 (q): Linear(in_features=1472, out_features=384, bias=False) [repeated 8x across cluster]
(GpuProver pid=877589)                 (k): Linear(in_features=1472, out_features=384, bias=False) [repeated 8x across cluster]
(GpuProver pid=877589)                 (v): Linear(in_features=1472, out_features=384, bias=False) [repeated 8x across cluster]
(GpuProver pid=877589)                 (o): Linear(in_features=384, out_features=1472, bias=False) [repeated 8x across cluster]
(GpuProver pid=877589)                 (relative_attention_bias): Embedding(32, 6) [repeated 4x across cluster]
(GpuProver pid=877589) ) [repeated 64x across cluster]
(GpuProver pid=877589)               (layer_norm): T5LayerNorm() [repeated 16x across cluster]
(GpuProver pid=877589)       (dropout): Dropout(p=0.1, inplace=False) [repeated 28x across cluster]
(GpuProver pid=877589)               (DenseReluDense): T5DenseGatedActDense( [repeated 8x across cluster]
(GpuProver pid=877589)                 (wi_1): Linear(in_features=1472, out_features=3584, bias=False) [repeated 16x across cluster]
(GpuProver pid=877589)                 (wo): Linear(in_features=3584, out_features=1472, bias=False) [repeated 8x across cluster]
(GpuProver pid=877589)                 (act): NewGELUActivation() [repeated 8x across cluster]
(GpuProver pid=877589)         (1-11): 11 x T5Block( [repeated 4x across cluster]
(GpuProver pid=877589)       (final_layer_norm): T5LayerNorm() [repeated 4x across cluster]
(GpuProver pid=877589) 2024-06-02 12:15:20.346 | INFO     | prover.proof_search:__init__:374 - Loading indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl [repeated 4x across cluster]
(GpuProver pid=877589) 2024-06-02 12:15:20.346 | INFO     | common:__init__:200 - Building the corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl [repeated 4x across cluster]
  0%|          | 1/3381 [00:00<50:33,  1.11it/s]
  0%|          | 4/3381 [00:01<11:44,  4.80it/s]
  0%|          | 6/3381 [00:01<08:07,  6.93it/s]
(GpuProver pid=877588) 2024-06-02 12:15:27.635 | INFO     | prover.proof_search:__init__:377 - Loaded indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl [repeated 4x across cluster]
(GpuProver pid=877588) 2024-06-02 12:15:27.636 | INFO     | retrieval.model:reindex_corpus:173 - Re-indexing the retrieval corpus [repeated 4x across cluster]
  0%|          | 0/3381 [00:00<?, ?it/s] [repeated 4x across cluster]
  0%|          | 10/3381 [00:05<34:52,  1.61it/s] [repeated 46x across cluster]
  1%|          | 27/3381 [00:11<25:40,  2.18it/s] [repeated 48x across cluster]
  1%|          | 35/3381 [00:15<13:23,  4.17it/s] [repeated 75x across cluster]
  1%|▏         | 46/3381 [00:20<22:03,  2.52it/s] [repeated 54x across cluster]
  2%|▏         | 54/3381 [00:25<36:12,  1.53it/s] [repeated 39x across cluster]
  2%|▏         | 61/3381 [00:30<39:34,  1.40it/s] [repeated 35x across cluster]
  2%|▏         | 72/3381 [00:36<41:00,  1.34it/s] [repeated 33x across cluster]
  2%|▏         | 81/3381 [00:41<16:11,  3.40it/s] [repeated 57x across cluster]
  3%|▎         | 98/3381 [00:46<33:28,  1.63it/s] [repeated 67x across cluster]
  3%|▎         | 101/3381 [00:51<33:27,  1.63it/s] [repeated 43x across cluster]
  3%|▎         | 111/3381 [00:56<24:27,  2.23it/s] [repeated 41x across cluster]
  4%|▍         | 129/3381 [01:02<21:55,  2.47it/s] [repeated 61x across cluster]
  4%|▍         | 132/3381 [01:06<23:59,  2.26it/s] [repeated 60x across cluster]
  4%|▍         | 148/3381 [01:12<35:04,  1.54it/s] [repeated 48x across cluster]
  5%|▍         | 156/3381 [01:17<31:00,  1.73it/s] [repeated 36x across cluster]
  5%|▍         | 165/3381 [01:22<33:15,  1.61it/s] [repeated 48x across cluster]
  5%|▍         | 169/3381 [01:26<35:05,  1.53it/s] [repeated 40x across cluster]
  5%|▌         | 175/3381 [01:31<37:38,  1.42it/s] [repeated 36x across cluster]
  5%|▌         | 183/3381 [01:37<41:29,  1.28it/s] [repeated 34x across cluster]
  6%|▌         | 188/3381 [01:42<42:29,  1.25it/s] [repeated 31x across cluster]
  6%|▌         | 196/3381 [01:47<42:07,  1.26it/s] [repeated 34x across cluster]
  6%|▌         | 202/3381 [01:52<36:45,  1.44it/s] [repeated 36x across cluster]
  6%|▋         | 214/3381 [01:58<40:46,  1.29it/s] [repeated 35x across cluster]
  6%|▋         | 217/3381 [02:03<41:48,  1.26it/s] [repeated 34x across cluster]
  7%|▋         | 222/3381 [02:08<40:29,  1.30it/s] [repeated 34x across cluster]
  7%|▋         | 230/3381 [02:13<42:01,  1.25it/s] [repeated 31x across cluster]
  7%|▋         | 236/3381 [02:18<37:43,  1.39it/s] [repeated 36x across cluster]
  7%|▋         | 244/3381 [02:23<40:24,  1.29it/s] [repeated 35x across cluster]
  8%|▊         | 256/3381 [02:29<33:04,  1.57it/s] [repeated 39x across cluster]
  8%|▊         | 258/3381 [02:33<33:10,  1.57it/s] [repeated 37x across cluster]
  8%|▊         | 270/3381 [02:39<40:09,  1.29it/s] [repeated 34x across cluster]
  8%|▊         | 273/3381 [02:44<41:06,  1.26it/s] [repeated 34x across cluster]
  8%|▊         | 278/3381 [02:49<40:47,  1.27it/s] [repeated 32x across cluster]
  8%|▊         | 285/3381 [02:54<33:18,  1.55it/s] [repeated 37x across cluster]
  9%|▊         | 293/3381 [02:59<40:54,  1.26it/s] [repeated 31x across cluster]
  9%|▉         | 298/3381 [03:04<41:15,  1.25it/s] [repeated 31x across cluster]
  9%|▉         | 304/3381 [03:09<41:15,  1.24it/s] [repeated 32x across cluster]
  9%|▉         | 311/3381 [03:14<40:31,  1.26it/s] [repeated 33x across cluster]
  9%|▉         | 317/3381 [03:19<41:02,  1.24it/s] [repeated 33x across cluster]
 10%|▉         | 325/3381 [03:24<40:59,  1.24it/s] [repeated 31x across cluster]
^C
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
main.py 591 <module>
main()

main.py 562 main
num_sorries, proofs, num_premises, num_files_traced, unproved_sorries = retrieve_proof(lean_git_repo, repo_no_dir, lean_git_repo.commit)

main.py 460 retrieve_proof
results = prover.search_unordered(repo, theorems, positions)

proof_search.py 475 search_unordered
results = list(

actor_pool.py 170 get_generator
yield self.get_next_unordered()

actor_pool.py 352 get_next_unordered
res, _ = ray.wait(list(self._future_to_actor), num_returns=1, timeout=timeout)

auto_init_hook.py 21 auto_init_wrapper
return fn(*args, **kwargs)

client_mode_hook.py 103 wrapper
return func(*args, **kwargs)

worker.py 2854 wait
ready_ids, remaining_ids = worker.core_worker.wait(

_raylet.pyx 3812 ray._raylet.CoreWorker.wait


_raylet.pyx 571 ray._raylet.check_status


KeyboardInterrupt
^CException ignored in atexit callback: <function shutdown at 0x7f02832b1fc0>
Traceback (most recent call last):
  File "/home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/ray/_private/worker.py", line 1813, in shutdown
    time.sleep(0.5)
KeyboardInterrupt:

^C
^C

(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$
(base) adarsh@tensorlab-DGX-Station-A100-920-23487-2531-000:~/ReProver$ bash run_lean_copilot_bot.sh
Script executed from: /home/adarsh/ReProver
[2024-06-02 12:19:12,073] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
 [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
 [WARNING]  async_io: please install the libaio-dev package with apt
 [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
 [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
Cloning https://github.com/lecopivo/SciLean.git
Repo name: lecopivo/SciLean
Deleting existing repository directory: /raid/adarsh/repos/lecopivo/SciLean
Cloning into '/raid/adarsh/repos/lecopivo/SciLean'...
remote: Enumerating objects: 14952, done.
remote: Counting objects: 100% (1010/1010), done.
remote: Compressing objects: 100% (296/296), done.
remote: Total 14952 (delta 824), reused 830 (delta 710), pack-reused 13942
Receiving objects: 100% (14952/14952), 9.48 MiB | 8.75 MiB/s, done.
Resolving deltas: 100% (10922/10922), done.
Cloning https://github.com/leanprover-community/aesop.git
Repo name: leanprover-community/aesop
Deleting existing repository directory: /raid/adarsh/repos/leanprover-community/aesop
Cloning into '/raid/adarsh/repos/leanprover-community/aesop'...
remote: Enumerating objects: 6936, done.
remote: Counting objects: 100% (1180/1180), done.
remote: Compressing objects: 100% (256/256), done.
remote: Total 6936 (delta 983), reused 1044 (delta 903), pack-reused 5756
Receiving objects: 100% (6936/6936), 3.34 MiB | 6.85 MiB/s, done.
Resolving deltas: 100% (5030/5030), done.
Cloning https://github.com/kmill/lean4-raytracer.git
Repo name: kmill/lean4-raytracer
Deleting existing repository directory: /raid/adarsh/repos/kmill/lean4-raytracer
Cloning into '/raid/adarsh/repos/kmill/lean4-raytracer'...
remote: Enumerating objects: 156, done.
remote: Counting objects: 100% (156/156), done.
remote: Compressing objects: 100% (93/93), done.
remote: Total 156 (delta 75), reused 128 (delta 52), pack-reused 0
Receiving objects: 100% (156/156), 10.06 MiB | 8.44 MiB/s, done.
Resolving deltas: 100% (75/75), done.
Cloning https://github.com/AlexKontorovich/PrimeNumberTheoremAnd.git
Repo name: AlexKontorovich/PrimeNumberTheoremAnd
Deleting existing repository directory: /raid/adarsh/repos/AlexKontorovich/PrimeNumberTheoremAnd
Cloning into '/raid/adarsh/repos/AlexKontorovich/PrimeNumberTheoremAnd'...
remote: Enumerating objects: 5560, done.
remote: Counting objects: 100% (2341/2341), done.
remote: Compressing objects: 100% (594/594), done.
remote: Total 5560 (delta 1831), reused 1913 (delta 1727), pack-reused 3219
Receiving objects: 100% (5560/5560), 1.56 MiB | 4.83 MiB/s, done.
Resolving deltas: 100% (3892/3892), done.
Cloning https://github.com/leanprover-community/lean-perfectoid-spaces.git
Repo name: leanprover-community/lean-perfectoid-spaces
Cloning into '/raid/adarsh/repos/leanprover-community/lean-perfectoid-spaces'...
remote: Enumerating objects: 6779, done.
remote: Counting objects: 100% (317/317), done.
remote: Compressing objects: 100% (107/107), done.
remote: Total 6779 (delta 197), reused 315 (delta 196), pack-reused 6462
Receiving objects: 100% (6779/6779), 12.00 MiB | 9.20 MiB/s, done.
Resolving deltas: 100% (4919/4919), done.
Failed to clone leanprover-community/lean-perfectoid-spaces because of HTTP Error 404: Not Found
Cloning https://github.com/flypitch/flypitch.git
Repo name: flypitch/flypitch
Cloning into '/raid/adarsh/repos/flypitch/flypitch'...
remote: Enumerating objects: 3574, done.
remote: Counting objects: 100% (121/121), done.
remote: Compressing objects: 100% (68/68), done.
remote: Total 3574 (delta 68), reused 85 (delta 53), pack-reused 3453
Receiving objects: 100% (3574/3574), 1.80 MiB | 4.05 MiB/s, done.
Resolving deltas: 100% (2689/2689), done.
Failed to clone flypitch/flypitch because of HTTP Error 404: Not Found
Cloning https://github.com/jesse-michael-han/lean-gptf.git
Repo name: jesse-michael-han/lean-gptf
Cloning into '/raid/adarsh/repos/jesse-michael-han/lean-gptf'...
remote: Enumerating objects: 266, done.
remote: Counting objects: 100% (22/22), done.
remote: Compressing objects: 100% (10/10), done.
remote: Total 266 (delta 14), reused 12 (delta 12), pack-reused 244
Receiving objects: 100% (266/266), 51.42 KiB | 435.00 KiB/s, done.
Resolving deltas: 100% (124/124), done.
Failed to clone jesse-michael-han/lean-gptf because of HTTP Error 404: Not Found
Cloning https://github.com/teorth/pfr.git
Repo name: teorth/pfr
Deleting existing repository directory: /raid/adarsh/repos/teorth/pfr
Cloning into '/raid/adarsh/repos/teorth/pfr'...
remote: Enumerating objects: 8182, done.
remote: Counting objects: 100% (285/285), done.
remote: Compressing objects: 100% (90/90), done.
remote: Total 8182 (delta 213), reused 255 (delta 194), pack-reused 7897
Receiving objects: 100% (8182/8182), 4.47 MiB | 6.82 MiB/s, done.
Resolving deltas: 100% (5908/5908), done.
Cloning https://github.com/TwoFX/sudoku.git
Repo name: TwoFX/sudoku
Cloning into '/raid/adarsh/repos/TwoFX/sudoku'...
remote: Enumerating objects: 1316, done.
remote: Counting objects: 100% (385/385), done.
remote: Compressing objects: 100% (151/151), done.
remote: Total 1316 (delta 254), reused 364 (delta 234), pack-reused 931
Receiving objects: 100% (1316/1316), 1.14 MiB | 3.59 MiB/s, done.
Resolving deltas: 100% (851/851), done.
Failed to clone TwoFX/sudoku because of HTTP Error 404: Not Found
Cloning https://github.com/blanchette/logical_verification_2020.git
Repo name: blanchette/logical_verification_2020
Cloning into '/raid/adarsh/repos/blanchette/logical_verification_2020'...
remote: Enumerating objects: 281, done.
remote: Counting objects: 100% (24/24), done.
remote: Compressing objects: 100% (20/20), done.
remote: Total 281 (delta 10), reused 18 (delta 4), pack-reused 257
Receiving objects: 100% (281/281), 5.57 MiB | 8.68 MiB/s, done.
Resolving deltas: 100% (165/165), done.
Failed to clone blanchette/logical_verification_2020 because of HTTP Error 404: Not Found
Cloning https://github.com/lurk-lab/yatima.git
Repo name: lurk-lab/yatima
Deleting existing repository directory: /raid/adarsh/repos/lurk-lab/yatima
Cloning into '/raid/adarsh/repos/lurk-lab/yatima'...
remote: Enumerating objects: 3500, done.
remote: Counting objects: 100% (273/273), done.
remote: Compressing objects: 100% (162/162), done.
remote: Total 3500 (delta 150), reused 196 (delta 109), pack-reused 3227
Receiving objects: 100% (3500/3500), 1.19 MiB | 4.39 MiB/s, done.
Resolving deltas: 100% (2209/2209), done.
2024-06-02 12:20:21.470 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/lurk-lab/yatima', commit='d9f20f51bca748878b8561661fe8bc19a7dba609') relies on an unsupported Lean version: c21d2f29a21a
d17351ebb7353fce9d7c7f6bdca7
Found 6 repositories
Processing /raid/adarsh/repos/lecopivo/SciLean
From https://github.com/lecopivo/SciLean
 * branch            master     -> FETCH_HEAD
Already on 'master'
Your branch is up to date with 'origin/master'.
From https://github.com/lecopivo/SciLean
 * branch            master     -> FETCH_HEAD
Already up to date.
Switched to a new branch '_LeanCopilotBot'
2024-06-02 12:20:22.295 | INFO     | __main__:retrieve_proof:383 - lean toolchain version: {'content': 'leanprover/lean4:v4.7.0\n'}
2024-06-02 12:20:22.295 | INFO     | __main__:retrieve_proof:385 - lean version v: v4.7.0
2024-06-02 12:20:22.295 | INFO     | __main__:retrieve_proof:386 - is supported: True
2024-06-02 12:20:22.295 | INFO     | __main__:retrieve_proof:392 - lean path /home/adarsh/.elan/toolchains/leanprover--lean4---4.7.0
2024-06-02 12:20:22.295 | INFO     | __main__:retrieve_proof:398 - Switched to Lean toolchain at: /home/adarsh/.elan/toolchains/leanprover--lean4---4.7.0
2024-06-02 12:20:22.337 | INFO     | __main__:retrieve_proof:399 - lean --version: Lean (version 4.7.0, x86_64-unknown-linux-gnu, commit 6fce8f7d5cd1, Release)

2024-06-02 12:20:22.337 | INFO     | __main__:retrieve_proof:400 - repo: LeanGitRepo(url='https://github.com/lecopivo/SciLean', commit='e21a3ecdc0442bf795ea665d3a4bdbc5be22602d')
2024-06-02 12:20:22.338 | INFO     | lean_dojo.data_extraction.trace:trace:116 - Loading the traced repo from /raid/adarsh/.cache/lean_dojo/lecopivo-SciLean-e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/SciLean
2024-06-02 12:20:24,710 INFO worker.py:1740 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8267
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 2522/2851 [03:56<00:07, 43.67it/s]
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2851/2851 [04:46<00:00,  9.95it/s]
Following Github server redirection from /repos/leanprover/std4 to /repositories/529900532
2024-06-02 12:25:45.278 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/leanprover/lean4-cli', commit='be8fa79a28b8b6897dce0713ef50e89c4a0f6ef5') relies on an unsupported Lean version: ec94173
5c80dc54c53948e30c428905b6600f95a
Following Github server redirection from /repos/mhuisi/lean4-cli to /repositories/341363356
2024-06-02 12:26:19.652 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/mhuisi/lean4-cli', commit='10d88b52fa8d717fa8e29af3abf0c3a2bf175497') relies on an unsupported Lean version: 41697dcf6ca
b7ec82723ba404f2bda7a4526bb2b
2024-06-02 12:26:37.165 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/xubaiw/CMark.lean', commit='0077cbbaa92abf855fc1c0413e158ffd8195ec77') relies on an unsupported Lean version: 8fc1af650a
d6d31cf766d9bc84119149330e7d4e
2024-06-02 12:26:46.762 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/fgdorais/lean4-unicode-basic', commit='280d75fdfe7be8eb337be7f1bf8479b4aac09f71') relies on an unsupported Lean version:
 0d7051497ea09b2b4a4ef608e371b8f317487c3c
2024-06-02 12:26:50.203 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/mhuisi/lean4-cli', commit='39229f3630d734af7d9cfb5937ddc6b41d3aa6aa') relies on an unsupported Lean version: 216d2460e0a
dec8317fdeeb6f2543cb7442564fd
2024-06-02 12:26:55.136 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:442 - LeanGitRepo(url='https://github.com/hargonix/LeanInk', commit='2447df5cc6e48eb965c3c3fba87e46d353b5e9f1') relies on an unsupported Lean version: f6cd6c06958
7cfe62dd68cb6330f9ad794a56724
2024-06-02 12:28:07.637 | INFO     | __main__:retrieve_proof:412 - MAIN: about to split data
2024-06-02 12:28:15.030 | INFO     | __main__:split_data:132 - 72244 theorems in total
2024-06-02 12:28:15.030 | INFO     | __main__:split_randomly:81 - Splitting the theorems randomly
2024-06-02 12:28:15.057 | INFO     | __main__:split_by_premise:94 - Splitting the theorems by premises
2024-06-02 12:28:20.825 | INFO     | __main__:retrieve_proof:414 - MAIN: done splitting data
2024-06-02 12:28:20.826 | INFO     | __main__:retrieve_proof:415 - MAIN: about to export corpus.jsonl
2024-06-02 12:28:20.826 | INFO     | __main__:export_data:179 - Exporting data to path: /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d
> /home/adarsh/ReProver/main.py(142)export_premises()
    141     import ipdb; ipdb.set_trace()
--> 142     oup_path = dst_path / repo_no_dir / sha
    143     corpus_file = oup_path / FILE_NAME

ipdb> with oup_path.open("wt") as oup:
        G = traced_repo.traced_files_graph

        for tf_node in reversed(

        list(nx.topological_sort(G))):
            tf = G.nodes[tf_node]["traced_file"]
            imports = [str(_) for _ in G.successors(tf_node)]
            premises = tf.get_premise_definitions()
            num_premises += len(premises)
            oup.write(
                json.dumps(
                    {"path": str(tf.path), "imports": imports, "premises": premises}
                )
                + "\n"
            )
    logger.info(
*** IndentationError: unindent does not match any outer indentation level
ipdb> n
> /home/adarsh/ReProver/main.py(143)export_premises()
    142     oup_path = dst_path / repo_no_dir / sha
--> 143     corpus_file = oup_path / FILE_NAME
    144     if corpus_file.exists():

ipdb>
> /home/adarsh/ReProver/main.py(144)export_premises()
    143     corpus_file = oup_path / FILE_NAME
--> 144     if corpus_file.exists():
    145         logger.info(f"{corpus_file} already exists. Using existing file")

ipdb>
> /home/adarsh/ReProver/main.py(148)export_premises()
    147         return 0, 0
--> 148     oup_path.mkdir(parents=True, exist_ok=True)
    149     num_premises = 0

ipdb> n
> /home/adarsh/ReProver/main.py(149)export_premises()
    148     oup_path.mkdir(parents=True, exist_ok=True)
--> 149     num_premises = 0
    150

ipdb> n
> /home/adarsh/ReProver/main.py(151)export_premises()
    150
--> 151     with corpus_file.open("wt") as oup:
    152         G = traced_repo.traced_files_graph

ipdb>
> /home/adarsh/ReProver/main.py(152)export_premises()
    151     with corpus_file.open("wt") as oup:
--> 152         G = traced_repo.traced_files_graph
    153

ipdb>
> /home/adarsh/ReProver/main.py(154)export_premises()
    153
--> 154         for tf_node in reversed(list(nx.topological_sort(G))):
    155             tf = G.nodes[tf_node]["traced_file"]

ipdb> c
2024-06-02 12:29:12.666 | INFO     | __main__:export_premises:165 - 108180 theorems/definitions from 2851 files saved to /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d
2024-06-02 12:29:12.666 | INFO     | __main__:retrieve_proof:417 - MAIN: exported corpus.jsonl
2024-06-02 12:29:20.185 | INFO     | __main__:retrieve_proof:434 - 28999
2024-06-02 12:29:20.200 | INFO     | __main__:retrieve_proof:448 - Found 293 sorries!
2024-06-02 12:29:20.201 | INFO     | __main__:retrieve_proof:449 - MAIN: about to search for proofs
2024-06-02 12:29:20.201 | INFO     | prover.proof_search:__init__:429 - Launching 5 workers with 1 GPUs.
2024-06-02 12:29:24,061 INFO worker.py:1740 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8267
2024-06-02 12:29:25.075 | INFO     | prover.proof_search:search_unordered:474 - before theorem search:
(pid=911371) [2024-06-02 12:29:29,987] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
(pid=911371)  [WARNING]  async_io requires the dev libaio .so object and headers but these were not found.
(pid=911371)  [WARNING]  async_io: please install the libaio-dev package with apt
(pid=911371)  [WARNING]  If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
(pid=911371)  [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
(pid=911371)  [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
(pid=911371)  [WARNING]  using untested triton version (2.3.0), only 1.0.0 is known to be compatible
(GpuProver pid=911371) Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../raid/adarsh/kaiyuy_lean
dojo-lean4-retriever-tacgen-byt5-small/model_lightning.ckpt`
(GpuProver pid=911371) 2024-06-02 12:29:31.774 | INFO     | generator.model:__init__:109 - Retriever checkpoint path: /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt
(GpuProver pid=911371) 2024-06-02 12:29:31.774 | INFO     | generator.model:__init__:124 - Loading the retriever from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt
(GpuProver pid=911371) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when p
ossible. If you want to force a new download, use `force_download=True`.
(GpuProver pid=911371)   warnings.warn(
(GpuProver pid=911371) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
(GpuProver pid=911371) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['encoder.shared.weight', 'encoder.encoder.em
bed_tokens.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.o.w
eight', 'encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.encoder.block.0.layer.0.layer_norm.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.0.layer.1.DenseRel
uDense.wi_1.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.0.layer.1.layer_norm.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.k.wei
ght', 'encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.1.layer.0.layer_norm.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encode
r.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.1.layer.1.layer_norm.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.encoder.b
lock.2.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.2.layer.0.layer_norm.weight', 'encoder.encoder.block.2.layer.1.De
nseReluDense.wi_0.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.2.layer.1.layer_norm.weight', 'encoder.encoder.block.3.layer.0.SelfAtten
tion.q.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.3.layer.0.layer_norm.weight', '
encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.3.layer.1.layer_norm.weight', 'encoder
.encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.encoder.blo
ck.4.layer.0.layer_norm.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.4.la
yer.1.layer_norm.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.5.layer.0.SelfAttenti
on.o.weight', 'encoder.encoder.block.5.layer.0.layer_norm.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wo.we
ight', 'encoder.encoder.block.5.layer.1.layer_norm.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.e
ncoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.6.layer.0.layer_norm.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.bl
ock.6.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.6.layer.1.layer_norm.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.7.layer.0.S
elfAttention.v.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.7.layer.0.layer_norm.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense
.wi_1.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.7.layer.1.layer_norm.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.k.weight',
'encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.8.layer.0.layer_norm.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.enco
der.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.8.layer.1.layer_norm.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.9
.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.9.layer.0.layer_norm.weight', 'encoder.encoder.block.9.layer.1.DenseRel
uDense.wi_0.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.9.layer.1.layer_norm.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.
q.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.10.layer.0.layer_norm.weight', 'e
ncoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.10.layer.1.layer_norm.weight', 'enco
der.encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.enco
der.block.11.layer.0.layer_norm.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.encoder
.block.11.layer.1.layer_norm.weight', 'encoder.encoder.final_layer_norm.weight']
(GpuProver pid=911371) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['shared.weight', 'decoder.embed_tokens.weigh
t', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.r
elative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.bl
ock.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight'
, 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight
', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention
.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.la
yer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.l
ayer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.
layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.bl
ock.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.b
lock.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', '
decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_h
ead.weight', 'pytorch-lightning_version', 'global_step', 'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'enco
der.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.la
yer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.laye
r.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.lay
er.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.
block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'enc
oder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight
', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi
_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttent
ion.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.D
enseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.
0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.blo
ck.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.bl
ock.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'enco
der.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'enc
oder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.we
ight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi
_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAt
tention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.D
enseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10
.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'en
coder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weig
ht', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.1.DenseR
eluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight']
(GpuProver pid=911372) Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at kaiyuy/leandojo-lean4-retriever-byt5-small and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decode
r.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.laye
r_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.lay
er.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block
.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.bloc
k.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'dec
oder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.we
ight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k
.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseRel
uDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer
.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.la
yer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder
.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight']
(GpuProver pid=911372) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(GpuProver pid=911372) 2024-06-02 12:29:34.328 | INFO     | generator.model:__init__:137 - RetrievalAugmentedGenerator initialized
(GpuProver pid=911371) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['retriever.encoder.shared.weight', 'retrieve
r.encoder.encoder.embed_tokens.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.v.weight',
'retriever.encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'retriever.encoder.encoder.block.0.layer.0.layer_norm.weight', 'retriever.encoder.enco
der.block.0.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.0.layer.1.layer_norm
.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.blo
ck.1.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.1.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_1.weight',
 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.1.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.2.layer.0
.SelfAttention.k.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.2.layer.0.layer_norm.weight', 'retriever.encoder.
encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.2.layer.1.layer_
norm.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder
.block.3.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.3.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weig
ht', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.3.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.4.lay
er.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.4.layer.0.layer_norm.weight', 'retriever.enco
der.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.4.layer.1.la
yer_norm.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'retriever.encoder.enc
oder.block.5.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.5.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.
weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.5.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.6
.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.6.layer.0.layer_norm.weight', 'retriever.
encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.6.layer.
1.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'retriever.encoder
.encoder.block.7.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.7.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.w
i_1.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.7.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.blo
ck.8.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.8.layer.0.layer_norm.weight', 'retrie
ver.encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.8.la
yer.1.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'retriever.enc
oder.encoder.block.9.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.9.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDen
se.wi_1.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.9.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'retriever.encoder.encode
r.block.10.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.10.layer.0.layer_norm.weight'
, 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encode
r.block.10.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.v.weight'
, 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.11.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.11.
layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.11.layer.1.layer_norm.weight', 'retriever.encoder.encoder.final_layer_norm.weight']
(GpuProver pid=911371) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['pytorch-lightning_version', 'global_step',
'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters']
(GpuProver pid=911371) 2024-06-02 12:29:35.967 | INFO     | prover.proof_search:__init__:370 - Loaded model from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning.ckpt
(GpuProver pid=911371) 2024-06-02 12:29:35.968 | INFO     | prover.proof_search:__init__:371 - Using retriever: PremiseRetriever(
(GpuProver pid=911371)   (encoder): T5EncoderModel(
(GpuProver pid=911371)     (shared): Embedding(384, 1472)
(GpuProver pid=911371)     (encoder): T5Stack(
(GpuProver pid=911371)       (embed_tokens): Embedding(384, 1472)
(GpuProver pid=911371)       (block): ModuleList(
(GpuProver pid=911371)         (0): T5Block(
(GpuProver pid=911371)           (layer): ModuleList(
(GpuProver pid=911371)             (0): T5LayerSelfAttention(
(GpuProver pid=911371)               (SelfAttention): T5Attention(
(GpuProver pid=911371)                 (q): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=911371)                 (k): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=911371)                 (v): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=911371)                 (o): Linear(in_features=384, out_features=1472, bias=False)
(GpuProver pid=911371)                 (relative_attention_bias): Embedding(32, 6)
(GpuProver pid=911371)               )
(GpuProver pid=911371)               (layer_norm): T5LayerNorm()
(GpuProver pid=911371)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)             )
(GpuProver pid=911371)             (1): T5LayerFF(
(GpuProver pid=911371)               (DenseReluDense): T5DenseGatedActDense(
(GpuProver pid=911371)                 (wi_0): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=911371)                 (wi_1): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=911371)                 (wo): Linear(in_features=3584, out_features=1472, bias=False)
(GpuProver pid=911371)                 (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)                 (act): NewGELUActivation()
(GpuProver pid=911371)               )
(GpuProver pid=911371)               (layer_norm): T5LayerNorm()
(GpuProver pid=911371)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)             )
(GpuProver pid=911371)           )
(GpuProver pid=911371)         )
(GpuProver pid=911371)         (1-11): 11 x T5Block(
(GpuProver pid=911371)           (layer): ModuleList(
(GpuProver pid=911371)             (0): T5LayerSelfAttention(
(GpuProver pid=911371)               (SelfAttention): T5Attention(
(GpuProver pid=911371)                 (q): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=911371)                 (k): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=911371)                 (v): Linear(in_features=1472, out_features=384, bias=False)
(GpuProver pid=911371)                 (o): Linear(in_features=384, out_features=1472, bias=False)
(GpuProver pid=911371)               )
(GpuProver pid=911371)               (layer_norm): T5LayerNorm()
(GpuProver pid=911371)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)             )
(GpuProver pid=911371)             (1): T5LayerFF(
(GpuProver pid=911371)               (DenseReluDense): T5DenseGatedActDense(
(GpuProver pid=911371)                 (wi_0): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=911371)                 (wi_1): Linear(in_features=1472, out_features=3584, bias=False)
(GpuProver pid=911371)                 (wo): Linear(in_features=3584, out_features=1472, bias=False)
(GpuProver pid=911371)                 (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)                 (act): NewGELUActivation()
(GpuProver pid=911371)               )
(GpuProver pid=911371)               (layer_norm): T5LayerNorm()
(GpuProver pid=911371)               (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)             )
(GpuProver pid=911371)           )
(GpuProver pid=911371)         )
(GpuProver pid=911371)       )
(GpuProver pid=911371)       (final_layer_norm): T5LayerNorm()
(GpuProver pid=911371)       (dropout): Dropout(p=0.1, inplace=False)
(GpuProver pid=911371)     )
(GpuProver pid=911371)   )
(GpuProver pid=911371) )
(GpuProver pid=911371) 2024-06-02 12:29:35.968 | INFO     | prover.proof_search:__init__:374 - Loading indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl
(GpuProver pid=911371) 2024-06-02 12:29:35.969 | INFO     | common:__init__:200 - Building the corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl
(GpuProver pid=911369) 2024-06-02 12:29:43.206 | INFO     | prover.proof_search:__init__:377 - Loaded indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl
(GpuProver pid=911369) 2024-06-02 12:29:43.206 | INFO     | retrieval.model:reindex_corpus:173 - Re-indexing the retrieval corpus
  0%|          | 0/3381 [00:00<?, ?it/s]
(GpuProver pid=911372) Lightning automatically upgraded your loaded checkpoint from v0.0.0 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../raid/adarsh/kaiyuy_lean
dojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt` [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/use
r-guides/configure-logging.html#log-deduplication for more options.)
(GpuProver pid=911367) 2024-06-02 12:29:32.018 | INFO     | generator.model:__init__:109 - Retriever checkpoint path: /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt [repeated 4x across cluster]
(GpuProver pid=911367) 2024-06-02 12:29:32.019 | INFO     | generator.model:__init__:124 - Loading the retriever from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning_retriever.ckpt [repeated 4x across cluster]
(GpuProver pid=911372) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when p
ossible. If you want to force a new download, use `force_download=True`. [repeated 4x across cluster]
(GpuProver pid=911372)   warnings.warn( [repeated 4x across cluster]
(GpuProver pid=911367) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. [repeated 9x across cluster]
(GpuProver pid=911366) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['encoder.shared.weight', 'encoder.encoder.em
bed_tokens.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.0.layer.0.SelfAttention.o.w
eight', 'encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.encoder.block.0.layer.0.layer_norm.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.0.layer.1.DenseRel
uDense.wi_1.weight', 'encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.0.layer.1.layer_norm.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.k.wei
ght', 'encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.1.layer.0.layer_norm.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encode
r.encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.1.layer.1.layer_norm.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.encoder.b
lock.2.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.2.layer.0.layer_norm.weight', 'encoder.encoder.block.2.layer.1.De
nseReluDense.wi_0.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.2.layer.1.layer_norm.weight', 'encoder.encoder.block.3.layer.0.SelfAtten
tion.q.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.3.layer.0.layer_norm.weight', '
encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.3.layer.1.layer_norm.weight', 'encoder
.encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.encoder.blo
ck.4.layer.0.layer_norm.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.4.la
yer.1.layer_norm.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.5.layer.0.SelfAttenti
on.o.weight', 'encoder.encoder.block.5.layer.0.layer_norm.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.5.layer.1.DenseReluDense.wo.we
ight', 'encoder.encoder.block.5.layer.1.layer_norm.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.e
ncoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.6.layer.0.layer_norm.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.bl
ock.6.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.6.layer.1.layer_norm.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.7.layer.0.S
elfAttention.v.weight', 'encoder.encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.7.layer.0.layer_norm.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense
.wi_1.weight', 'encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.7.layer.1.layer_norm.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.k.weight',
'encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.8.layer.0.layer_norm.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.enco
der.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.8.layer.1.layer_norm.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.9
.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.9.layer.0.layer_norm.weight', 'encoder.encoder.block.9.layer.1.DenseRel
uDense.wi_0.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.9.layer.1.layer_norm.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.
q.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.encoder.block.10.layer.0.layer_norm.weight', 'e
ncoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.encoder.block.10.layer.1.layer_norm.weight', 'enco
der.encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.enco
der.block.11.layer.0.layer_norm.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.encoder
.block.11.layer.1.layer_norm.weight', 'encoder.encoder.final_layer_norm.weight'] [repeated 4x across cluster]
(GpuProver pid=911366) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['shared.weight', 'decoder.embed_tokens.weigh
t', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.r
elative_attention_bias.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.bl
ock.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight'
, 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight
', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention
.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.la
yer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.l
ayer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.
layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.bl
ock.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.b
lock.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', '
decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_h
ead.weight', 'pytorch-lightning_version', 'global_step', 'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters', 'encoder.embed_tokens.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'enco
der.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.la
yer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.laye
r.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.lay
er.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.
block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'enc
oder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight
', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi
_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttent
ion.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.D
enseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.
0.SelfAttention.o.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.blo
ck.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.bl
ock.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'enco
der.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'enc
oder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.we
ight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi
_0.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAt
tention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.9.layer.1.D
enseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10
.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'en
coder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weig
ht', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.1.DenseR
eluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.final_layer_norm.weight'] [repeated 4x across cluster]
(GpuProver pid=911366) Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at kaiyuy/leandojo-lean4-retriever-byt5-small and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decode
r.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.laye
r_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.lay
er.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block
.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.bloc
k.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'dec
oder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.we
ight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k
.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseRel
uDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer
.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.la
yer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder
.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'lm_head.weight'] [repeated 4x across cluster]
(GpuProver pid=911366) You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. [repeated 4x across cluster]
(GpuProver pid=911366) 2024-06-02 12:29:34.351 | INFO     | generator.model:__init__:137 - RetrievalAugmentedGenerator initialized [repeated 4x across cluster]
(GpuProver pid=911369) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:184: Found keys that are in the model state dict but not in the checkpoint: ['retriever.encoder.shared.weight', 'retrieve
r.encoder.encoder.embed_tokens.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.v.weight',
'retriever.encoder.encoder.block.0.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'retriever.encoder.encoder.block.0.layer.0.layer_norm.weight', 'retriever.encoder.enco
der.block.0.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.0.layer.1.layer_norm
.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.1.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.blo
ck.1.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.1.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wi_1.weight',
 'retriever.encoder.encoder.block.1.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.1.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.2.layer.0
.SelfAttention.k.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.2.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.2.layer.0.layer_norm.weight', 'retriever.encoder.
encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.2.layer.1.layer_
norm.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.3.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder
.block.3.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.3.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wi_1.weig
ht', 'retriever.encoder.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.3.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.4.lay
er.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.4.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.4.layer.0.layer_norm.weight', 'retriever.enco
der.encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.4.layer.1.la
yer_norm.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.5.layer.0.SelfAttention.v.weight', 'retriever.encoder.enc
oder.block.5.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.5.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wi_1.
weight', 'retriever.encoder.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.5.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.6
.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.6.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.6.layer.0.layer_norm.weight', 'retriever.
encoder.encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.6.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.6.layer.
1.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.7.layer.0.SelfAttention.v.weight', 'retriever.encoder
.encoder.block.7.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.7.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.w
i_1.weight', 'retriever.encoder.encoder.block.7.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.7.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.blo
ck.8.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.8.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.8.layer.0.layer_norm.weight', 'retrie
ver.encoder.encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.8.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.8.la
yer.1.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.9.layer.0.SelfAttention.v.weight', 'retriever.enc
oder.encoder.block.9.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.9.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDen
se.wi_1.weight', 'retriever.encoder.encoder.block.9.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.9.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.q.weight', 'retriever.encoder.encode
r.block.10.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.v.weight', 'retriever.encoder.encoder.block.10.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.10.layer.0.layer_norm.weight'
, 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.10.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encode
r.block.10.layer.1.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.q.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.k.weight', 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.v.weight'
, 'retriever.encoder.encoder.block.11.layer.0.SelfAttention.o.weight', 'retriever.encoder.encoder.block.11.layer.0.layer_norm.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'retriever.encoder.encoder.block.11.
layer.1.DenseReluDense.wi_1.weight', 'retriever.encoder.encoder.block.11.layer.1.DenseReluDense.wo.weight', 'retriever.encoder.encoder.block.11.layer.1.layer_norm.weight', 'retriever.encoder.encoder.final_layer_norm.weight'] [repeated 4x across
 cluster]
(GpuProver pid=911369) /home/adarsh/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['pytorch-lightning_version', 'global_step',
'epoch', 'state_dict', 'callbacks', 'loops', 'legacy_pytorch-lightning_version', 'hyper_parameters'] [repeated 4x across cluster]
(GpuProver pid=911372) 2024-06-02 12:29:36.051 | INFO     | prover.proof_search:__init__:370 - Loaded model from /raid/adarsh/kaiyuy_leandojo-lean4-retriever-tacgen-byt5-small/model_lightning.ckpt [repeated 4x across cluster]
(GpuProver pid=911372) 2024-06-02 12:29:36.053 | INFO     | prover.proof_search:__init__:371 - Using retriever: PremiseRetriever( [repeated 4x across cluster]
(GpuProver pid=911372)     (encoder): T5Stack( [repeated 8x across cluster]
(GpuProver pid=911372)     (shared): Embedding(384, 1472) [repeated 4x across cluster]
(GpuProver pid=911372)       (embed_tokens): Embedding(384, 1472) [repeated 4x across cluster]
(GpuProver pid=911372)       (block): ModuleList( [repeated 4x across cluster]
(GpuProver pid=911372)             (1): T5LayerFF( [repeated 20x across cluster]
(GpuProver pid=911372)           (layer): ModuleList( [repeated 8x across cluster]
(GpuProver pid=911372)               (SelfAttention): T5Attention( [repeated 8x across cluster]
(GpuProver pid=911372)                 (q): Linear(in_features=1472, out_features=384, bias=False) [repeated 8x across cluster]
(GpuProver pid=911372)                 (k): Linear(in_features=1472, out_features=384, bias=False) [repeated 8x across cluster]
(GpuProver pid=911372)                 (v): Linear(in_features=1472, out_features=384, bias=False) [repeated 8x across cluster]
(GpuProver pid=911372)                 (o): Linear(in_features=384, out_features=1472, bias=False) [repeated 8x across cluster]
(GpuProver pid=911372)                 (relative_attention_bias): Embedding(32, 6) [repeated 4x across cluster]
(GpuProver pid=911372) ) [repeated 64x across cluster]
(GpuProver pid=911372)               (layer_norm): T5LayerNorm() [repeated 16x across cluster]
(GpuProver pid=911372)       (dropout): Dropout(p=0.1, inplace=False) [repeated 28x across cluster]
(GpuProver pid=911372)               (DenseReluDense): T5DenseGatedActDense( [repeated 8x across cluster]
(GpuProver pid=911372)                 (wi_1): Linear(in_features=1472, out_features=3584, bias=False) [repeated 16x across cluster]
(GpuProver pid=911372)                 (wo): Linear(in_features=3584, out_features=1472, bias=False) [repeated 8x across cluster]
(GpuProver pid=911372)                 (act): NewGELUActivation() [repeated 8x across cluster]
(GpuProver pid=911372)         (1-11): 11 x T5Block( [repeated 4x across cluster]
(GpuProver pid=911372)       (final_layer_norm): T5LayerNorm() [repeated 4x across cluster]
(GpuProver pid=911372) 2024-06-02 12:29:36.053 | INFO     | prover.proof_search:__init__:374 - Loading indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl [repeated 4x across cluster]
(GpuProver pid=911372) 2024-06-02 12:29:36.053 | INFO     | common:__init__:200 - Building the corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl [repeated 4x across cluster]
  0%|          | 1/3381 [00:00<46:56,  1.20it/s]
(GpuProver pid=911371) 2024-06-02 12:29:43.352 | INFO     | prover.proof_search:__init__:377 - Loaded indexed corpus from /raid/adarsh/data/lecopivo/SciLean/e21a3ecdc0442bf795ea665d3a4bdbc5be22602d/corpus.jsonl [repeated 4x across cluster]
(GpuProver pid=911371) 2024-06-02 12:29:43.353 | INFO     | retrieval.model:reindex_corpus:173 - Re-indexing the retrieval corpus [repeated 4x across cluster]
  0%|          | 0/3381 [00:00<?, ?it/s] [repeated 4x across cluster]
  0%|          | 13/3381 [00:06<33:52,  1.66it/s] [repeated 53x across cluster]
  1%|          | 24/3381 [00:11<21:11,  2.64it/s] [repeated 51x across cluster]
  1%|          | 36/3381 [00:16<22:37,  2.46it/s] [repeated 71x across cluster]
  1%|▏         | 49/3381 [00:21<26:00,  2.14it/s] [repeated 60x across cluster]
  2%|▏         | 56/3381 [00:26<39:06,  1.42it/s] [repeated 35x across cluster]

